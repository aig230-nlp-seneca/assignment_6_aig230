{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef871424",
   "metadata": {},
   "source": [
    "# AIG230 – Assignment 6 (Part B Starter Notebook)\n",
    "## Neural Language Model with PyTorch (RNN) – Student Version\n",
    "\n",
    "This notebook covers **all of Part B**:\n",
    "\n",
    "- **B1** Numericalization + training examples  \n",
    "- **B2** Build an RNN Language Model  \n",
    "- **B3** Train + validate (loss + perplexity)  \n",
    "- **B4** Test perplexity + text generation  \n",
    "\n",
    "### Dataset (same as Part A)\n",
    "- **NLTK Brown corpus**, category: `news`\n",
    "\n",
    "### Important\n",
    "This is a **starter notebook**. You must complete the **TODO** blocks.  \n",
    "Do not delete TODO comments. Add your code underneath them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188e0d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 1) Setup =====\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Download if needed (safe to run multiple times)\n",
    "nltk.download('brown')\n",
    "\n",
    "# Reproducibility (optional)\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e38f176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 2) Configuration (edit if needed) =====\n",
    "# You can edit these hyperparameters as needed. The defaults should work for a basic run.\n",
    "# This are not fixed rules, just a starting point. Feel free to experiment with different values!\n",
    "# This parameter class is just a convenient way to store all the settings in one place. You can also use a dictionary or command-line arguments if you prefer.\n",
    "# These parameters define the dataset category, how to split the data, vocabulary cutoff, model architecture, and training settings. Adjusting these can affect the performance and training time of your model.\n",
    "@dataclass\n",
    "class Config:\n",
    "    category: str = \"news\"\n",
    "    train_ratio: float = 0.80\n",
    "    val_ratio: float = 0.10\n",
    "    test_ratio: float = 0.10\n",
    "\n",
    "    min_freq: int = 2       # vocab cutoff (train only)\n",
    "    seq_len: int = 30       # T\n",
    "    batch_size: int = 32\n",
    "\n",
    "    emb_dim: int = 128\n",
    "    hid_dim: int = 256\n",
    "    num_layers: int = 1\n",
    "    dropout: float = 0.0    # use 0.0 if num_layers == 1\n",
    "\n",
    "    lr: float = 1e-3\n",
    "    epochs: int = 5\n",
    "    grad_clip: float = 1.0  # optional but recommended\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "SPECIAL = {\"BOS\": \"<bos>\", \"EOS\": \"<eos>\", \"UNK\": \"<unk>\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a879346a",
   "metadata": {},
   "source": [
    "# 3) Load + preprocess Brown (shared rules)\n",
    "\n",
    "Preprocessing rules:\n",
    "- lowercase\n",
    "- remove punctuation-only tokens\n",
    "- keep stopwords\n",
    "- add `<bos>` and `<eos>` to each sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6adca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 3) Load Brown sentences =====\n",
    "# This section loads the raw sentences from the Brown corpus, preprocesses them by lowercasing and removing punctuation-only tokens, and adds special tokens for the beginning and end of sentences. The resulting `sents` variable is a list of tokenized sentences ready for building the vocabulary and training the model.\n",
    "raw_sents: List[List[str]] = brown.sents(categories=cfg.category)\n",
    "print(\"Raw sentences:\", len(raw_sents))\n",
    "print(\"Example raw:\", raw_sents[0][:20])\n",
    "\n",
    "_punct_only = re.compile(r\"^\\W+$\")\n",
    "\n",
    "def preprocess_sentence(tokens: List[str]) -> List[str]:\n",
    "    out = []\n",
    "    for tok in tokens:\n",
    "        tok = tok.lower()\n",
    "        if _punct_only.match(tok):\n",
    "            continue\n",
    "        out.append(tok)\n",
    "    return [SPECIAL[\"BOS\"], *out, SPECIAL[\"EOS\"]]\n",
    "\n",
    "sents = [preprocess_sentence(s) for s in raw_sents]\n",
    "print(\"Example preprocessed:\", sents[0][:25])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae9e2e9",
   "metadata": {},
   "source": [
    "# 4) Split train/val/test (by sentence)\n",
    "\n",
    "Split by sentence to avoid leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c0aa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 4) Split =====\n",
    "# This section splits the preprocessed sentences into training, validation, and test sets based on the specified ratios in the configuration. It also defines a helper function to count the total number of tokens in each split and prints out the number of sentences and tokens for each set.\n",
    "n = len(sents)\n",
    "n_train = int(cfg.train_ratio * n)\n",
    "n_val = int(cfg.val_ratio * n)\n",
    "n_test = n - n_train - n_val\n",
    "\n",
    "train_sents = sents[:n_train]\n",
    "val_sents = sents[n_train:n_train+n_val]\n",
    "test_sents = sents[n_train+n_val:]\n",
    "\n",
    "def num_tokens(slist: List[List[str]]) -> int:\n",
    "    return sum(len(s) for s in slist)\n",
    "\n",
    "print(\"Train:\", len(train_sents), \"sentences |\", num_tokens(train_sents), \"tokens\")\n",
    "print(\"Val  :\", len(val_sents),   \"sentences |\", num_tokens(val_sents),   \"tokens\")\n",
    "print(\"Test :\", len(test_sents),  \"sentences |\", num_tokens(test_sents),  \"tokens\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f45b99",
   "metadata": {},
   "source": [
    "# 5) Build vocabulary (train only)\n",
    "\n",
    "Words with frequency `< min_freq` become `<unk>`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0291da50",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_sents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ===== 5) Vocabulary =====\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# This section builds the vocabulary from the training sentences by counting the frequency of each token and including only those that meet the minimum frequency threshold specified in the configuration. It also creates mappings from tokens to indices (`stoi`) and from indices to tokens (`itos`), and prints out some statistics about the vocabulary and the most common tokens in the training set.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m train_tokens = [tok \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_sents\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m sent]\n\u001b[32m      4\u001b[39m counts = Counter(train_tokens)\n\u001b[32m      6\u001b[39m itos = [SPECIAL[\u001b[33m\"\u001b[39m\u001b[33mBOS\u001b[39m\u001b[33m\"\u001b[39m], SPECIAL[\u001b[33m\"\u001b[39m\u001b[33mEOS\u001b[39m\u001b[33m\"\u001b[39m], SPECIAL[\u001b[33m\"\u001b[39m\u001b[33mUNK\u001b[39m\u001b[33m\"\u001b[39m]]\n",
      "\u001b[31mNameError\u001b[39m: name 'train_sents' is not defined"
     ]
    }
   ],
   "source": [
    "# ===== 5) Vocabulary =====\n",
    "# This section builds the vocabulary from the training sentences by counting the frequency of each token and including only those that meet the minimum frequency threshold specified in the configuration. \n",
    "# It also creates mappings from tokens to indices (`stoi`) and from indices to tokens (`itos`), and prints out some statistics about the vocabulary and the most common tokens in the training set.\n",
    "train_tokens = [tok for sent in train_sents for tok in sent]\n",
    "counts = Counter(train_tokens)\n",
    "\n",
    "itos = [SPECIAL[\"BOS\"], SPECIAL[\"EOS\"], SPECIAL[\"UNK\"]]\n",
    "stoi: Dict[str, int] = {tok: i for i, tok in enumerate(itos)}\n",
    "\n",
    "for tok, c in counts.most_common():\n",
    "    if tok in stoi:\n",
    "        continue\n",
    "    if c >= cfg.min_freq:\n",
    "        stoi[tok] = len(itos)\n",
    "        itos.append(tok)\n",
    "\n",
    "vocab_size = len(itos)\n",
    "unk_id = stoi[SPECIAL[\"UNK\"]]\n",
    "\n",
    "print(\"min_freq:\", cfg.min_freq)\n",
    "print(\"vocab_size:\", vocab_size)\n",
    "print(\"UNK id:\", unk_id)\n",
    "print(\"\\nTop 15 tokens (train):\")\n",
    "for tok, c in counts.most_common(15):\n",
    "    print(f\"{tok:>12}  {c}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9342ae",
   "metadata": {},
   "source": [
    "# 6) B1 – Numericalize + create training examples (Option 1)\n",
    "\n",
    "You will:\n",
    "1) Convert each sentence into token IDs  \n",
    "2) Concatenate into one long stream per split  \n",
    "3) Build a Dataset that returns:\n",
    "- `x = stream[i : i+T]`\n",
    "- `y = stream[i+1 : i+T+1]`\n",
    "\n",
    "Complete the TODO blocks below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f7cd10",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'List' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ===== 6.1) Numericalize (TODO) =====\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# This function converts a list of tokens into a list of corresponding token IDs using the `stoi` mapping. If a token is not found in the vocabulary, it uses the `unk_id` to represent it. This step is essential for preparing the data to be fed into the model, as models typically work with numerical representations of text.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnumericalize_sentence\u001b[39m(tokens: \u001b[43mList\u001b[49m[\u001b[38;5;28mstr\u001b[39m], stoi: Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m], unk_id: \u001b[38;5;28mint\u001b[39m) -> List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# TODO: return list of token IDs for this sentence\u001b[39;00m\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# Hint: use stoi.get(tok, unk_id)\u001b[39;00m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[32m      9\u001b[39m train_ids_sents = [numericalize_sentence(s, stoi, unk_id) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m train_sents]\n",
      "\u001b[31mNameError\u001b[39m: name 'List' is not defined"
     ]
    }
   ],
   "source": [
    "# ===== 6.1) Numericalize (TODO) =====\n",
    "# This function converts a list of tokens into a list of corresponding token IDs using the `stoi` mapping. \n",
    "# If a token is not found in the vocabulary, it uses the `unk_id` to represent it. This step is essential for preparing the data to be fed into the model, as models typically work with numerical representations of text.\n",
    "# Why this is neccessary? Models cannot process raw text; they require numerical input. By converting tokens to their corresponding IDs, we can efficiently represent the text data in a format suitable for training neural networks. This also allows us to handle out-of-vocabulary tokens gracefully using the `unk_id`.\n",
    "# How this connect to the embedding layer? The embedding layer takes token IDs as input and maps them to dense vector representations (embeddings). \n",
    "# By numericalizing the sentences, we can feed these token IDs into the embedding layer, which will then produce the corresponding embeddings for each token. This is a crucial step in the pipeline, as it allows us to convert raw text into a format that can be processed by the neural network.\n",
    "def numericalize_sentence(tokens: List[str], stoi: Dict[str, int], unk_id: int) -> List[int]:\n",
    "    # TODO: return list of token IDs for this sentence\n",
    "    # Hint: use stoi.get(tok, unk_id)\n",
    "    raise NotImplementedError\n",
    "\n",
    "train_ids_sents = [numericalize_sentence(s, stoi, unk_id) for s in train_sents]\n",
    "val_ids_sents   = [numericalize_sentence(s, stoi, unk_id) for s in val_sents]\n",
    "test_ids_sents  = [numericalize_sentence(s, stoi, unk_id) for s in test_sents]\n",
    "\n",
    "print(\"Example tokens:\", train_sents[0][:12])\n",
    "print(\"Example ids   :\", train_ids_sents[0][:12])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab5bcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 6.2) Build streams =====\n",
    "train_stream = [tid for sent in train_ids_sents for tid in sent]\n",
    "val_stream   = [tid for sent in val_ids_sents   for tid in sent]\n",
    "test_stream  = [tid for sent in test_ids_sents  for tid in sent]\n",
    "\n",
    "print(\"Train stream length:\", len(train_stream))\n",
    "print(\"Val   stream length:\", len(val_stream))\n",
    "print(\"Test  stream length:\", len(test_stream))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b22a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 6.3) Dataset (TODO) =====\n",
    "class NextTokenStreamDataset(Dataset):\n",
    "    def __init__(self, token_stream: List[int], seq_len: int):\n",
    "        self.stream = token_stream\n",
    "        self.T = seq_len\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # TODO: return number of examples in this stream\n",
    "        # Need T tokens for x and 1 extra token for y shift.\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # TODO: create x and y slices, convert to torch.long tensors\n",
    "        # x: stream[idx : idx+T]\n",
    "        # y: stream[idx+1 : idx+T+1]\n",
    "        raise NotImplementedError\n",
    "\n",
    "train_ds = NextTokenStreamDataset(train_stream, cfg.seq_len)\n",
    "val_ds   = NextTokenStreamDataset(val_stream,   cfg.seq_len)\n",
    "test_ds  = NextTokenStreamDataset(test_stream,  cfg.seq_len)\n",
    "\n",
    "print(\"Train examples:\", len(train_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95cddc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 6.4) DataLoaders + sanity check (run after TODOs are done) =====\n",
    "train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, drop_last=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=cfg.batch_size, shuffle=False, drop_last=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=cfg.batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "x_batch, y_batch = next(iter(train_loader))\n",
    "print(\"x_batch shape:\", x_batch.shape)  # expected: (B, T)\n",
    "print(\"y_batch shape:\", y_batch.shape)  # expected: (B, T)\n",
    "print(\"x_batch dtype:\", x_batch.dtype)\n",
    "\n",
    "print(\"First 10 x:\", x_batch[0][:10].tolist())\n",
    "print(\"First 10 y:\", y_batch[0][:10].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934b1ed0",
   "metadata": {},
   "source": [
    "# 7) B2 – Build the RNN Language Model\n",
    "\n",
    "Model requirements:\n",
    "- Embedding layer\n",
    "- RNN layer (`nn.RNN`)\n",
    "- Linear layer to vocab size\n",
    "\n",
    "Complete the TODO blocks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8af687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 7) Model (TODO) =====\n",
    "# Configuration class is defined at the top of the notebook. You can adjust the hyperparameters there as needed.\n",
    "class RNNLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_dim: int, hid_dim: int, num_layers: int = 1, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        # TODO: define embedding layer\n",
    "        # TODO: define RNN layer (batch_first=True)\n",
    "        # TODO: define output projection layer (hid_dim -> vocab_size)\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, T) token IDs\n",
    "        # Return logits: (B, T, V)\n",
    "        # TODO: embed -> rnn -> linear\n",
    "        raise NotImplementedError\n",
    "\n",
    "# Instantiate model (after TODOs)\n",
    "model = RNNLanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    emb_dim=cfg.emb_dim,\n",
    "    hid_dim=cfg.hid_dim,\n",
    "    num_layers=cfg.num_layers,\n",
    "    dropout=cfg.dropout\n",
    ").to(device)\n",
    "\n",
    "# Parameter count\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Model parameters:\", num_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44751d8f",
   "metadata": {},
   "source": [
    "# 8) B3 – Training + validation (loss + perplexity)\n",
    "\n",
    "You will:\n",
    "- Define loss (`CrossEntropyLoss`)\n",
    "- Train for several epochs\n",
    "- Compute validation perplexity\n",
    "\n",
    "Complete the TODO blocks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ba1a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 8.1) Loss + optimizer (TODO) =====\n",
    "# For language modeling with logits of shape (B, T, V),\n",
    "# CrossEntropyLoss expects (N, V) logits and (N,) targets.\n",
    "criterion = None  # TODO: set nn.CrossEntropyLoss()\n",
    "optimizer = None  # TODO: set torch.optim.Adam(model.parameters(), lr=cfg.lr)\n",
    "\n",
    "print(\"Ready (criterion, optimizer):\", criterion is not None, optimizer is not None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0dc9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 8.2) Helper: compute perplexity (TODO) =====\n",
    "@torch.no_grad()\n",
    "def evaluate_perplexity(model: nn.Module, loader: DataLoader) -> float:\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # TODO:\n",
    "        # 1) logits = model(x) -> (B, T, V)\n",
    "        # 2) reshape logits to (B*T, V)\n",
    "        # 3) reshape y to (B*T,)\n",
    "        # 4) loss = criterion(...)\n",
    "        # 5) accumulate total_loss weighted by number of tokens\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "    avg_loss = total_loss / max(total_tokens, 1)\n",
    "    ppl = math.exp(avg_loss)\n",
    "    return ppl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69929f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 8.3) Training loop (TODO) =====\n",
    "def train_one_epoch(model: nn.Module, loader: DataLoader) -> float:\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_tokens = 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # TODO:\n",
    "        # 1) optimizer.zero_grad()\n",
    "        # 2) logits = model(x)\n",
    "        # 3) reshape logits/y for CrossEntropyLoss\n",
    "        # 4) loss.backward()\n",
    "        # 5) optional: clip gradients\n",
    "        # 6) optimizer.step()\n",
    "        # 7) accumulate running_loss weighted by number of tokens\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return running_loss / max(running_tokens, 1)\n",
    "\n",
    "train_losses = []\n",
    "val_ppls = []\n",
    "\n",
    "for epoch in range(1, cfg.epochs + 1):\n",
    "    train_loss = train_one_epoch(model, train_loader)\n",
    "    val_ppl = evaluate_perplexity(model, val_loader)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_ppls.append(val_ppl)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | train loss: {train_loss:.4f} | val ppl: {val_ppl:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fffa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 8.4) Plot training loss (optional) =====\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(1, len(train_losses)+1), train_losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Train Loss\")\n",
    "plt.title(\"Training Loss per Epoch\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21a8699",
   "metadata": {},
   "source": [
    "# 9) B4 – Test perplexity + text generation\n",
    "\n",
    "You will:\n",
    "- Report test perplexity\n",
    "- Generate text by sampling from the model\n",
    "\n",
    "Complete the TODO blocks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67cc420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 9.1) Test perplexity =====\n",
    "test_ppl = evaluate_perplexity(model, test_loader)  # will work after TODOs done\n",
    "print(\"Test perplexity:\", test_ppl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bb85f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 9.2) Text generation (TODO) =====\n",
    "# We will generate tokens one-by-one:\n",
    "# - Start with <bos>\n",
    "# - Predict next token distribution\n",
    "# - Sample (or argmax)\n",
    "# - Append token and continue\n",
    "\n",
    "def sample_next_token(logits_1v: torch.Tensor, temperature: float = 1.0) -> int:\n",
    "    # logits_1v: (V,) logits for next token\n",
    "    # TODO:\n",
    "    # 1) divide logits by temperature\n",
    "    # 2) convert to probabilities (softmax)\n",
    "    # 3) sample an index using torch.multinomial\n",
    "    raise NotImplementedError\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_text(model: nn.Module, stoi: Dict[str, int], itos: List[str],\n",
    "                  max_new_tokens: int = 50, temperature: float = 1.0) -> str:\n",
    "    model.eval()\n",
    "\n",
    "    bos_id = stoi[SPECIAL[\"BOS\"]]\n",
    "    eos_id = stoi[SPECIAL[\"EOS\"]]\n",
    "\n",
    "    # Start sequence with <bos>\n",
    "    generated = [bos_id]\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        # TODO:\n",
    "        # 1) create input tensor x of shape (1, current_len)\n",
    "        # 2) logits = model(x) -> (1, current_len, V)\n",
    "        # 3) take last timestep logits: logits[0, -1, :]\n",
    "        # 4) sample next token id\n",
    "        # 5) append; break if eos\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # Convert ids to tokens and return as a string\n",
    "    tokens = [itos[i] for i in generated]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "print(generate_text(model, stoi, itos, max_new_tokens=60, temperature=1.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd66fdd8",
   "metadata": {},
   "source": [
    "# What you submit for Part B\n",
    "\n",
    "In your final submission, include:\n",
    "- Your completed code for all TODO blocks\n",
    "- Your training loss plot\n",
    "- Validation perplexity per epoch (printed)\n",
    "- Final test perplexity\n",
    "- 3 generated samples (30+ tokens each) with brief comments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16cc151",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
